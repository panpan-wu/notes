## 总体结构

```
app statsd client 写入-> statsd_exporter <-抓取 本机vector 写入-> 中心vector
node_exporter <-抓取 本机vector 写入-> 中心vector
app syslog client 写入-> 本机vector 写入-> 中心vector 写入-> 本地磁盘
中心vector <-抓取 prometheus 写入-> 远程prometheus
```

### statsd_exporter

```bash
wget https://github.com/prometheus/statsd_exporter/releases/download/v0.24.0/statsd_exporter-0.24.0.linux-amd64.tar.gz
tar -xzvf statsd_exporter-0.24.0.linux-amd64.tar.gz
cp statsd_exporter-0.24.0.linux-amd64/statsd_exporter /usr/local/bin
```

/etc/systemd/system/statsd_exporter.service

```
[Unit]
Description=statsd_exporter
Wants=network-online.target
After=network-online.target

[Service]
User=root
Group=root
Type=simple
ExecStart=/usr/local/bin/statsd_exporter \
    --statsd.mapping-config=/etc/statsd_exporter/statsd_mapping.yml \
    --web.listen-address=":9102" \
    --statsd.listen-udp=":9125" \
    --statsd.listen-tcp=":9125"
ExecReload=/usr/bin/kill -HUP $MAINPID

[Install]
WantedBy=multi-user.target
```

/etc/statsd_exporter/statsd_mapping.yml

```yaml
defaults:
  observer_type: histogram
  histogram_options:
    buckets: [.005, .01, .025, .05, .1, .15, 0.2, .25, .3, .5, 1, 2.5]
    native_histogram_bucket_factor: 1.1
    native_histogram_max_buckets: 256
  summary_options:
    quantiles:
      - quantile: 0.99
        error: 0.001
      - quantile: 0.95
        error: 0.01
      - quantile: 0.9
        error: 0.05
      - quantile: 0.5
        error: 0.005
    max_age: 5m
    age_buckets: 2
    buf_cap: 1000
  match_type: glob
  glob_disable_ordering: false
  ttl: 0 # metrics do not expire
```

```bash
systemctl enable statsd_exporter
systemctl start statsd_exporter
```

### node_exporter

```bash
wget https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-amd64.tar.gz
tar -xzvf node_exporter-1.7.0.linux-amd64.tar.gz
cp node_exporter-1.7.0.linux-amd64/node_exporter /usr/local/bin
```

/etc/systemd/system/node_exporter.service

```
[Unit]
Description=NodeExporter
Wants=network-online.target
After=network-online.target

[Service]
User= root
Group= root
Type=simple
ExecStart=/usr/local/bin/node_exporter \
    --collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)

[Install]
WantedBy=multi-user.target
```

```bash
systemctl enable node_exporter
systemctl start node_exporter
```

### 本机 vector

在 ubuntu 安装

```bash
apt update
curl -1sLf 'https://repositories.timber.io/public/vector/cfg/setup/bash.deb.sh' | bash
apt install vector
```

/lib/systemd/system/vector.service

```
[Unit]
Description=Vector
Documentation=https://vector.dev
After=network-online.target
Requires=network-online.target

[Service]
User=root
Group=root
ExecStartPre=/usr/bin/vector validate /etc/vector/vector.toml
ExecStart=/usr/bin/vector -c /etc/vector/vector.toml
ExecReload=/usr/bin/vector validate /etc/vector/vector.toml
ExecReload=/bin/kill -HUP $MAINPID
Restart=always
AmbientCapabilities=CAP_NET_BIND_SERVICE
EnvironmentFile=-/etc/default/vector
# Since systemd 229, should be in [Unit] but in order to support systemd <229,
# it is also supported to have it here.
StartLimitInterval=10
StartLimitBurst=5
[Install]
WantedBy=multi-user.target
```

```bash
systemctl enable vector
systemctl start vector
```

/etc/vector/vector.toml

```toml
# Change this to use a non-default directory for Vector data storage:
# data_dir = "/var/lib/vector"

# syslog begin
[sources.syslog]
type = "syslog"
mode = "tcp"
address = "127.0.0.1:1514"

[transforms.syslog_remap]
type = "remap"
inputs = ["syslog"]
source = '''
.hostname = del(.host)
del(.facility)
del(.procid)
del(.msgid)
del(.severity)
del(.source_ip)
del(.source_type)
del(.version)
.tag = .appname
del(.appname)
.app_name = split!(.tag, ".", limit: 2)[0]
'''

[sinks.syslog_forward]
type = "vector"
inputs = ["syslog_remap"]
address = "192.168.35.0:8001"
version = "2"
# syslog end


# prometheus begin
[sources.prometheus_scrape]
type = "prometheus_scrape"
endpoints = ["http://127.0.0.1:9102/metrics"]
scrape_interval_secs = 15

[transforms.prometheus_filter]
type = "filter"
inputs = ["prometheus_scrape"]
condition = '''
starts_with(to_string!(.name), "statsd_") == false &&
starts_with(to_string!(.name), "go_") == false &&
starts_with(to_string!(.name), "process_") == false &&
starts_with(to_string!(.name), "promhttp_") == false
'''

[transforms.prometheus_remap]
type = "remap"
inputs = ["prometheus_filter"]
source = '''
.tags.hostname = get_hostname!()
'''

# [sinks.prometheus_console]
# type = "console"
# inputs = ["prometheus_remap"]
# encoding.codec = "json"

[sinks.prometheus_forward]
type = "vector"
inputs = ["prometheus_remap"]
address = "192.168.35.0:9000"
version = "2"
# prometheus end

# prometheus node_exporter start
[sources.prometheus_node_exporter]
type = "prometheus_scrape"
endpoints = ["http://127.0.0.1:9100/metrics"]
scrape_interval_secs = 15

[transforms.prometheus_node_exporter_filter]
type = "filter"
inputs = ["prometheus_node_exporter"]
condition = '''
includes([
    "node_boot_time_seconds",
    "node_cpu_seconds_total",
    "node_disk_read_bytes_total",
    "node_disk_written_bytes_total",
    "node_filesystem_avail_bytes",
    "node_filesystem_free_bytes",
    "node_filesystem_size_bytes",
    "node_filesystem_size_bytes",
    "node_memory_MemAvailable_bytes",
    "node_memory_MemTotal_bytes",
    "node_netstat_Tcp_CurrEstab",
    "node_network_receive_bytes_total",
    "node_network_transmit_bytes_total",
    "node_uname_info",
    "node_load1",
    "node_load5",
    "node_load15",
    "node_filesystem_readonly"
], .name)
'''

[transforms.prometheus_node_exporter_remap]
type = "remap"
inputs = ["prometheus_node_exporter_filter"]
source = '''
.tags.hostname = get_hostname!()
'''

[sinks.prometheus_node_exporter_forward]
type = "vector"
inputs = ["prometheus_node_exporter_remap"]
address = "192.168.35.0:9000"
version = "2"
# prometheus node_exporter end
```

### 中心 vector

/etc/vector/vector.toml

```toml
# Change this to use a non-default directory for Vector data storage:
# data_dir = "/var/lib/vector"

# k8s begin 接收 k8s log
[sources.k8s]
type = "vector"
address = "0.0.0.0:8000"
version = "2"

[sinks.k8s_file]
type = "file"
inputs = ["k8s"]
path = "/mnt/log/k8s_log/{{ .k8s.pod_namespace }}/{{ app_name }}/{{ tag }}-%Y-%m-%d.log"
compression = "none"
encoding.codec = "json"
# k8s end


# ecs begin 接收 ecs log
[sources.ecs]
type = "vector"
address = "0.0.0.0:8001"
version = "2"

[transforms.ecs_route]
type = "route"
inputs = ["ecs"]
[transforms.ecs_route.route]
every_hour = 'includes([], .app_name)'

[sinks.ecs_file_every_hour]
type = "file"
inputs = ["ecs_route.every_hour"]
encoding.codec = "json"
compression = "none"
path = "/mnt/log/ecs_log/{{ app_name }}/{{ app_name }}-%Y-%m-%d-%H.log"

[sinks.ecs_file_every_day]
type = "file"
inputs = ["ecs_route._unmatched"]
encoding.codec = "json"
compression = "none"
path = "/mnt/log/ecs_log/{{ app_name }}/{{ app_name }}-%Y-%m-%d.log"
# ecs end


# ecs metrics begin 接收 prometheus 指标
[sources.ecs_metrics]
type = "vector"
address = "0.0.0.0:9000"
version = "2"

[sinks.ecs_metrics_prometheus_exporter]
type = "prometheus_exporter"
inputs = ["ecs_metrics"]
address = "127.0.0.1:9001"
flush_period_secs = 120
# ecs metrics end
```

log 压缩与清理

/opt/scripts/clear_and_gzip_log.sh

```bash
#!/bin/bash

find /var/log/vector -mtime +5 -name "*.gz" -o -name "*.log" -exec rm {} \;
find /var/log/vector -mtime +1 -name "*.log" -exec gzip {} \;
```

crontab -e

```
0 10 * * * /opt/scripts/clear_and_gzip_log.sh >> /opt/scripts/clear_and_gzip_log.log 2>&1
```

### prometheus

安装

```bash
wget https://github.com/prometheus/prometheus/releases/download/v2.48.1/prometheus-2.48.1.linux-amd64.tar.gz
tar -xzvf prometheus-2.48.1.linux-amd64.tar.gz
cp prometheus-2.48.1.linux-amd64/prometheus /usr/local/bin
mkdir /etc/prometheus
cp -R prometheus-2.48.1.linux-amd64/console_libraries /etc/prometheus
cp -R prometheus-2.48.1.linux-amd64/consoles /etc/prometheus
```

/etc/systemd/system/prometheus.service

```
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=root
Group=root
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.enable-lifecycle
ExecReload=/usr/bin/kill -HUP $MAINPID

[Install]
WantedBy=multi-user.target
```

```bash
systemctl enable prometheus
systemctl start prometheus
```

/etc/prometheus/prometheus.yml

```yaml
# my global config
global:
  scrape_interval: 60s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
# alerting:
#   alertmanagers:
#     - static_configs:
#         - targets:
#           # - alertmanager:9093

remote_write:
  - url: "xxx"
    basic_auth:
      username: "xxx"
      password: "xxx"

remote_read:
  - url: "xxx"
    read_recent: true
    basic_auth:
      username: "xxx"
      password: "xxx"
    headers:
      region: "xxx"
      service: "xxx"

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  - job_name: vector
    metrics_path: '/metrics'
    static_configs:
      - targets: [
          '127.0.0.1:9001'
        ]
    metric_relabel_configs:
      - source_labels: [hostname]
        target_label: instance
```
